---
title: "Multinomial Logistic Regression (MultiLR)"
subtitle: "STA 210 - Spring 2022"
author: "Dr. Mine Ã‡etinkaya-Rundel"
footer: "[sta210-s22.github.io/website](https://sta210-s22.github.io/website/)"
logo: "images/logo.png"
format: 
  revealjs:
    theme: slides.scss
    transition: fade
    slide-number: true
    incremental: true 
    chalkboard: true
editor: visual
execute:
  freeze: auto
  echo: true
---

```{r}
#| include: false

# figure options
knitr::opts_chunk$set(
  fig.width = 8, fig.asp = 0.618, out.width = "90%",
  fig.retina = 3, dpi = 300, fig.align = "center"
)

library(countdown)
```

# Welcome

## Topics

::: nonincremental
-   Introduce multinomial logistic regression

-   Interpret model coefficients

-   Inference for a coefficient $\beta_{jk}$
:::

## Computational setup

```{r}
#| echo: true

# load packages
library(tidyverse)
library(tidymodels)
library(NHANES)
library(knitr)
library(patchwork)

# set default theme and larger font size for ggplot2
ggplot2::theme_set(ggplot2::theme_minimal(base_size = 20))
```

# Generalized Linear Models

## Generalized Linear Models (GLMs) {.smaller}

-   In practice, there are many different types of outcome variables:

    ::: nonincremental
    -   **Binary**: Win or Lose
    -   **Nominal**: Democrat, Republican or Third Party candidate
    -   **Ordered**: Movie rating (1 - 5 stars)
    -   and others...
    :::

-   Predicting each of these outcomes requires a **generalized linear model**, a broader class of models that *generalize* the multiple linear regression model

::: callout-note
Recommended reading for more details about GLMs: [*Generalized Linear Models: A Unifying Theory*](https://bookdown.org/roback/bookdown-bysh/ch-glms.html).
:::

## Binary outcome (Logistic)

-   Given $P(y_i=1|x_i)= \hat{\pi}_i\hspace{5mm} \text{ and } \hspace{5mm}P(y_i=0|x_i) = 1-\hat{\pi}_i$

    $$
    \log\Big(\frac{\hat{\pi}_i}{1-\hat{\pi}_i}\Big) = \hat{\beta}_0 + \hat{\beta}_1 x_{i}
    $$

-   We can calculate $\hat{\pi}_i$ by solving the logit equation:

    $$
    \hat{\pi}_i = \frac{e^{\hat{\beta}_0 + \hat{\beta}_1 x_{i}}}{1 + e^{\hat{\beta}_0 + \hat{\beta}_1 x_{i}}}
    $$

## Binary outcome (Logistic) {.smaller}

-   Suppose we consider $y=0$ the **baseline category** such that

    $$
    P(y_i=1|x_i) = \hat{\pi}_{i1} \hspace{2mm}  \text{ and } \hspace{2mm} P(y_i=0|x_i) = \hat{\pi}_{i0}
    $$

-   Then the logistic regression model is

    $$
    \log\bigg(\frac{\hat{\pi}_{i1}}{1- \hat{\pi}_{i1}}\bigg) = \log\bigg(\frac{\hat{\pi}_{i1}}{\hat{\pi}_{i0}}\bigg) = \hat{\beta}_0 + \hat{\beta}_1 x_i
    $$

-   Slope, $\hat{\beta}_1$: When $x$ increases by one unit, the odds of $y=1$ versus the baseline $y=0$ are expected to multiply by a factor of $e^{\hat{\beta}_1}$

-   Intercept, $\hat{\beta}_0$: When $x=0$, the predicted odds of $y=1$ versus the baseline $y=0$ are $\exp\{\hat{\beta}_0\}$

## Multinomial outcome variable

-   Suppose the outcome variable $y$ is categorical and can take values $1, 2, \ldots, K$ such that $(K > 2)$

-   **Multinomial Distribution:**

    $$
    P(y=1) = \pi_1, P(y=2) = \pi_2, \ldots, P(y=K) = \pi_K
    $$

    such that $\sum\limits_{k=1}^{K} \pi_k = 1$

## Multinomial Logistic Regression {.smaller}

-   If we have an explanatory variable $x$, then we want to fit a model such that $P(y = k) = \pi_k$ is a function of $x$

-   Choose a baseline category.
    Let's choose $y=1$.
    Then,

    $$
    \log\bigg(\frac{\pi_{ik}}{\pi_{i1}}\bigg) = \beta_{0k} + \beta_{1k} x_i
    $$

-   In the multinomial logistic model, we have a separate equation for each category of the outcome relative to the baseline category

    -   If the outcome has $K$ possible categories, there will be $K-1$ equations as part of the multinomial logistic model

## Multinomial Logistic Regression

-   Suppose we have a outcome variable $y$ that can take three possible outcomes that are coded as "A", "B", "C"

-   Let "A" be the baseline category.
    Then

    $$
    \log\bigg(\frac{\pi_{iB}}{\pi_{iA}}\bigg) = \beta_{0B} + \beta_{1B}x_i \\[10pt]
    \log\bigg(\frac{\pi_{iC}}{\pi_{iA}}\bigg) = \beta_{0C} + \beta_{1C} x_i
    $$

# Data

## NHANES Data

-   [National Health and Nutrition Examination Survey](https://www.cdc.gov/nchs/nhanes/index.htm) is conducted by the National Center for Health Statistics (NCHS)

-   The goal is to *"assess the health and nutritional status of adults and children in the United States"*

-   This survey includes an interview and a physical examination

## NHANES Data

-   We will use the data from the `NHANES` R package

-   Contains 75 variables for the 2009 - 2010 and 2011 - 2012 sample years

-   The data in this package is modified for educational purposes and should **not** be used for research

-   Original data can be obtained from the [NCHS website](https://www.cdc.gov/nchs/data_access/index.htm) for research purposes

-   Type `?NHANES` in console to see list of variables and definitions

## Variables

**Goal:** Use a person's age and whether they do regular physical activity to predict their self-reported health rating.

-   Outcome: `HealthGen`: Self-reported rating of participant's health in general.
    Excellent, Vgood, Good, Fair, or Poor.

-   Predictors:

    -   `Age`:Age at time of screening (in years). Participants 80 or older were recorded as 80.
    -   `PhysActive`: Participant does moderate to vigorous-intensity sports, fitness or recreational activities.

## The data

```{r}
nhanes_adult <- NHANES %>%
  filter(Age >= 18) %>%
  select(HealthGen, Age, PhysActive) %>%
  drop_na() %>%
  mutate(obs_num = 1:n())
```

. . .

```{r}
glimpse(nhanes_adult)
```

## Exploratory data analysis

```{r}
#| echo: false
#| out-width: "80%"
#| fig-width: 12

p1 <- ggplot(data = nhanes_adult, aes(x = Age)) + 
  geom_histogram(binwidth = 2) +
  labs(title = "Distribution of Age")

p2 <- ggplot(data = nhanes_adult, aes(x = PhysActive)) + 
  geom_bar() +
  labs(title = "Moderate or vigorous\nsport or exercise")

p3 <- ggplot(data = nhanes_adult, aes(y = HealthGen)) + 
  geom_bar() +
  labs(title = "Self-reported rating\nof overall health")

p3 + (p1 / p2)
```

## Exploratory data analysis

```{r}
#| echo: false
#| out-width: "80%"
#| fig-width: 12

p1 <- ggplot(data = nhanes_adult, aes(x = HealthGen, y = Age)) +
  geom_boxplot(fill = "#D9E3E4") + 
  labs(title = "Age vs.\nHealth Rating") +
  coord_flip()

p2 <- ggplot(data = nhanes_adult, aes(x = PhysActive, fill = HealthGen)) +
  geom_bar(position = "fill") +
  labs(y = "Proportion", 
       title = "Physical Activity vs.\nHealth Rating")

p1 + p2
```

# Fitting a multinomial logistic regression

## Model in R

Use the `multinom_reg()` function with the `"nnet"` engine:

```{r}
health_fit <- multinom_reg() %>%
  set_engine("nnet") %>%
  fit(HealthGen ~ Age + PhysActive, data = nhanes_adult)
```

## Model result

```{r}
health_fit
```

## Next steps

::: question
What function do we use to get the model summary, i.e., coefficient estimates.
:::

. . .

```{r}
#| error: true

tidy(health_fit)
```

## Looking inside the result of `fit()`

::: question
What is the name of the dataset in the call?
Is it right?
:::

```{r}
health_fit$fit$call
```

## Repair, and get back on track

```{r}
health_fit <- repair_call(health_fit, data = nhanes_adult)
health_fit$fit$call
tidy(health_fit)
```

## Model output

```{r}
tidy(health_fit)
```

## Model output, with CI

```{r}
tidy(health_fit, conf.int = TRUE)
```

## Model output, with CI {.smaller}

```{r}
#| echo: false
tidy(health_fit, conf.int = TRUE) %>%
  kable(digits = 3)
```

## Fair vs. Excellent Health

The baseline category for the model is `Excellent`.

. . .

The model equation for the log-odds a person rates themselves as having "Fair" health vs. "Excellent" is

$$
\log\Big(\frac{\hat{\pi}_{Fair}}{\hat{\pi}_{Excellent}}\Big) = 0.915  + 0.003 ~ \text{age} - 1.645 ~ \text{PhysActive}
$$

## Interpretations {.smaller}

$$
\log\Big(\frac{\hat{\pi}_{Fair}}{\hat{\pi}_{Excellent}}\Big) = 0.915  + 0.003 ~ \text{age} - 1.645 ~ \text{PhysActive}
$$

For each additional year in age, the odds a person rates themselves as having fair health versus excellent health are expected to multiply by `r round(exp(0.003), 3)` (exp(0.003)), holding physical activity constant.

. . .

The odds a person who does physical activity will rate themselves as having fair health versus excellent health are expected to be `r round(exp(-1.645 ),3)` (exp(-1.645)) times the odds for a person who doesn't do physical activity, holding age constant.

## Interpretations

$$
\log\Big(\frac{\hat{\pi}_{Fair}}{\hat{\pi}_{Excellent}}\Big) = 0.915  + 0.003 ~ \text{age} - 1.645 ~ \text{PhysActive}
$$

The odds a 0 year old person who doesn't do physical activity rates themselves as having fair health vs. excellent health are `r round(exp(0.915),3)` (exp(0.915)).

. . .

`r emo::ji("warning")` **Need to mean-center age for the intercept to have a meaningful interpretation!**

## Hypothesis test for $\beta_{jk}$

The test of significance for the coefficient $\beta_{jk}$ is

**Hypotheses**: $H_0: \beta_{jk} = 0 \hspace{2mm} \text{ vs } \hspace{2mm} H_a: \beta_{jk} \neq 0$

**Test Statistic**: $$z = \frac{\hat{\beta}_{jk} - 0}{SE(\hat{\beta_{jk}})}$$

**P-value**: $P(|Z| > |z|)$,

where $Z \sim N(0, 1)$, the Standard Normal distribution

## Confidence interval for $\beta_{jk}$

-   We can calculate the **C% confidence interval** for $\beta_{jk}$ using $\hat{\beta}_{jk} \pm z^* SE(\hat{\beta}_{jk})$, where $z^*$ is calculated from the $N(0,1)$ distribution.

-   We are $C\%$ confident that for every one unit change in $x_{j}$, the odds of $y = k$ versus the baseline will multiply by a factor of $\exp\{\hat{\beta}_{jk} - z^* SE(\hat{\beta}_{jk})\}$ to $\exp\{\hat{\beta}_{jk} + z^* SE(\hat{\beta}_{jk})\}$, holding all else constant.

## Interpreting CIs for $\beta_{jk}$ {.smaller}

```{r}
tidy(health_fit, conf.int = TRUE) %>%
  filter(y.level == "Fair") %>%
  kable(digits = 3)
```

<br>

. . .

We are 95% confident, that for each additional year in age, the odds a person rates themselves as having fair health versus excellent health will multiply by `r round(exp(-0.003), 3)` (exp(-0.003)) to `r round(exp(0.009), 3)` (exp(0.009)) , holding physical activity constant.

## Interpreting CIs for $\beta_{jk}$ {.smaller}

```{r}
tidy(health_fit, conf.int = TRUE) %>%
  filter(y.level == "Fair") %>%
  kable(digits = 3)
```

<br>

We are 95% confident that the odds a person who does physical activity will rate themselves as having fair health versus excellent health are `r round(exp(-1.856   ),3)` (exp(-1.856 )) to `r round(exp(-1.435),3)` (exp(-1.435)) times the odds for a person who doesn't do physical activity, holding age constant.

## Recap

-   Introduce multinomial logistic regression

-   Interpret model coefficients

-   Inference for a coefficient $\beta_{jk}$
